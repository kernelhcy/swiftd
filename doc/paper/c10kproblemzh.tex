\documentclass[12pt, twoside, a4paper, xetex]{report}
\usepackage{hcypaperstyle}

\begin{document}

\title{{\Huge C10K 问题\\}}
\author{黄丛宇\\06161032\\指导老师：马瑞芳}
\date{\today}

\lfour

\chapter{The C10K problem}

It's time for web servers to handle ten thousand clients simultaneously, don't you think? After all, the web is a big place now.

And computers are big, too. You can buy a 1000MHz machine with 2 gigabytes of RAM and an 1000Mbit/sec Ethernet card for \$1200 or so. Let's see - at 20000 clients, that's 50KHz, 100Kbytes, and 50Kbits/sec per client. It shouldn't take any more horsepower than that to take four kilobytes from the disk and send them to the network once a second for each of twenty thousand clients. (That works out to \$0.08 per client, by the way. Those \$100/client licensing fees some operating systems charge are starting to look a little heavy!) So hardware is no longer the bottleneck.

In 1999 one of the busiest ftp sites, cdrom.com, actually handled 10000 clients simultaneously through a Gigabit Ethernet pipe. As of 2001, that same speed is now being offered by several ISPs, who expect it to become increasingly popular with large business customers.

And the thin client model of computing appears to be coming back in style -- this time with the server out on the Internet, serving thousands of clients.

With that in mind, here are a few notes on how to configure operating systems and write code to support thousands of clients. The discussion centers around Unix-like operating systems, as that's my personal area of interest, but Windows is also covered a bit.

\section{Related Sites}

In October 2003, Felix von Leitner put together an excellent web page and presentation about network scalability, complete with benchmarks comparing various networking system calls and operating systems. One of his observations is that the 2.6 Linux kernel really does beat the 2.4 kernel, but there are many, many good graphs that will give the OS developers food for thought for some time. (See also the Slashdot comments; it'll be interesting to see whether anyone does followup benchmarks improving on Felix's results.)

\section{Book to Read First}

If you haven't read it already, go out and get a copy of Unix Network Programming : Networking Apis: Sockets and Xti (Volume 1) by the late W. Richard Stevens. It describes many of the I/O strategies and pitfalls related to writing high-performance servers. It even talks about the 'thundering herd' problem. And while you're at it, go read Jeff Darcy's notes on high-performance server design.

(Another book which might be more helpful for those who are *using* rather than *writing* a web server is Building Scalable Web Sites by Cal Henderson.)

\section{I/O frameworks}

Prepackaged libraries are available that abstract some of the techniques presented below, insulating your code from the operating system and making it more portable.
	
	\begin{enumerate}
	
	\item ACE, a heavyweight C++ I/O framework, contains object-oriented implementations of some of these I/O 		
	strategies and many other useful things. In particular, his Reactor is an OO way of doing nonblocking I/O, and 
	Proactor is an OO way of doing asynchronous I/O.
	
	\item ASIO is an C++ I/O framework which is becoming part of the Boost library. It's like ACE updated for the STL era.
	\item libevent is a lightweight C I/O framework by Niels Provos. It supports kqueue and select, and soon will 
	support poll and epoll. It's level-triggered only, I think, which has both good and bad sides. Niels has a nice 
	graph of time to handle one event as a function of the number of connections. It shows kqueue and sys\_epoll as 
	clear winners.
	\item My own attempts at lightweight frameworks (sadly, not kept up to date):
		
		\begin{enumerate}
		\item Poller is a lightweight C++ I/O framework that implements a level-triggered readiness API using 
		whatever underlying readiness API you want (poll, select, /dev/poll, kqueue, or sigio). It's useful for 
		benchmarks that compare the performance of the various APIs. This document links to Poller subclasses below 
		to illustrate how each of the readiness APIs can be used.
		\item rn is a lightweight C I/O framework that was my second try after Poller. It's lgpl (so it's easier to 
		use in commercial apps) and C (so it's easier to use in non-C++ apps). It was used in some commercial 
		products.
		\end{enumerate}
		
	\item Matt Welsh wrote a paper in April 2000 about how to balance the use of worker thread and event-driven 
	techniques when building scalable servers. The paper describes part of his Sandstorm I/O framework.
	\item Cory Nelson's Scale! library - an async socket, file, and pipe I/O library for Windows
	
	\end{enumerate}

\section{I/O Strategies}

Designers of networking software have many options. Here are a few:

	\begin{enumerate}
	\item Whether and how to issue multiple I/O calls from a single thread
		\begin{enumerate}
		\item Don't; use blocking/synchronous calls throughout, and possibly use multiple \\threads or processes to 
		    achieve concurrency
		\item Use nonblocking calls (e.g. write() on a socket set to O\_NONBLOCK) to start I/O, and readiness 
		    notification (e.g. poll() or /dev/poll) to know when it's OK to start the next I/O on that channel. 
		    Generally only usable with network I/O, not disk I/O.
		\item Use asynchronous calls (e.g. aio\_write()) to start I/O, and completion notification (e.g. signals or 
			completion ports) to know when the I/O finishes. Good for both network and disk I/O.
		
		\end{enumerate}
	\item How to control the code servicing each client
		\begin{enumerate}
		\item one process for each client (classic Unix approach, used since 1980 or so)
		\item one OS-level thread handles many clients; each client is controlled by:
			
			\begin{itemize}
			\item a user-level thread (e.g. GNU state threads, classic Java with green \\threads)
			\item a state machine (a bit esoteric, but popular in some circles; my favorite)
			\item a continuation (a bit esoteric, but popular in some circles)
			\end{itemize}
		\item one OS-level thread for each client (e.g. classic Java with native threads)
		\item one OS-level thread for each active client (e.g. Tomcat with apache front end; NT completion ports; 
			thread pools)
		\end{enumerate}
	\item Whether to use standard O/S services, or put some code into the kernel (e.g. in a custom driver, kernel 
		module, or VxD)
	\end{enumerate}
	
The following five combinations seem to be popular:

	\begin{itemize}
	\item Serve many clients with each thread, and use nonblocking I/O and level-triggered readiness notification
	\item Serve many clients with each thread, and use nonblocking I/O and readiness change notification
	\item Serve many clients with each server thread, and use asynchronous I/O
	\item serve one client with each server thread, and use blocking I/O
	\item Build the server code into the kernel
	\end{itemize}
\subsection{Serve many clients with each thread, and use nonblocking I/O and level-triggered readiness notification}

... set nonblocking mode on all network handles, and use select() or poll() to tell which network handle has data waiting. This is the traditional favorite. With this scheme, the kernel tells you whether a file descriptor is ready, whether or not you've done anything with that file descriptor since the last time the kernel told you about it. (The name 'level triggered' comes from computer hardware design; it's the opposite of 'edge triggered'. Jonathon Lemon introduced the terms in his BSDCON 2000 paper on kqueue().)

Note: it's particularly important to remember that readiness notification from the kernel is only a hint; the file descriptor might not be ready anymore when you try to read from it. That's why it's important to use nonblocking mode when using readiness notification.

An important bottleneck in this method is that read() or sendfile() from disk blocks if the page is not in core at the moment; setting nonblocking mode on a disk file handle has no effect. Same thing goes for memory-mapped disk files. The first time a server needs disk I/O, its process blocks, all clients must wait, and that raw nonthreaded performance goes to waste. 

This is what asynchronous I/O is for, but on systems that lack AIO, worker threads or processes that do the disk I/O can also get around this bottleneck. One approach is to use memory-mapped files, and if mincore() indicates I/O is needed, ask a worker to do the I/O, and continue handling network traffic. Jef Poskanzer mentions that Pai, Druschel, and Zwaenepoel's 1999 Flash web server uses this trick; they gave a talk at Usenix '99 on it. It looks like mincore() is available in BSD-derived Unixes like FreeBSD and Solaris, but is not part of the Single Unix Specification. It's available as part of Linux as of kernel 2.3.51, thanks to Chuck Lever.

But in November 2003 on the freebsd-hackers list, Vivek Pei et al reported very good results using system-wide profiling of their Flash web server to attack bottlenecks. One bottleneck they found was mincore (guess that wasn't such a good idea after all) Another was the fact that sendfile blocks on disk access; they improved performance by introducing a modified sendfile() that return something like EWOULDBLOCK when the disk page it's fetching is not yet in core. (Not sure how you tell the user the page is now resident... seems to me what's really needed here is aio\_sendfile().) The end result of their optimizations is a SpecWeb99 score of about 800 on a 1GHZ/1GB FreeBSD box, which is better than anything on file at spec.org.

There are several ways for a single thread to tell which of a set of nonblocking sockets are ready for I/O:

	\begin{enumerate}
	\item The traditional select() 
	
	Unfortunately, select() is limited to FD\_SETSIZE handles. This limit is compiled in to the standard library and 
	
	user programs. (Some versions of the C library let you raise this limit at user app compile time.)
	
	See Poller\_select (cc, h) for an example of how to use select() interchangeably with other readiness 
	notification schemes.

	\item  The traditional poll()
	 
	There is no hardcoded limit to the number of file descriptors poll() can handle, but it does get slow about a 
	few thousand, since most of the file descriptors are idle at any one time, and scanning through thousands of 
	file descriptors takes time.
	
	Some OS's (e.g. Solaris 8) speed up poll() et al by use of techniques like poll hinting, which was implemented 
	and benchmarked by Niels Provos for Linux in 1999.

	See Poller\_poll (cc, h, benchmarks) for an example of how to use poll() interchangeably with other readiness 
	notification schemes.

	\item /dev/poll
	
	This is the recommended poll replacement for Solaris.
	
	The idea behind /dev/poll is to take advantage of the fact that often poll() is called many times with the same 
	arguments. With /dev/poll, you get an open handle to /dev/poll, and tell the OS just once what files you're 
	interested in by writing to that handle; from then on, you just read the set of currently ready file
	descriptors from that handle.

	It appeared quietly in Solaris 7 (see patchid 106541) but its first public appearance was in Solaris 8; 	
	according to Sun, at 750 clients, this has 10% of the overhead of poll().

	Various implementations of /dev/poll were tried on Linux, but none of them perform as well as epoll, and were 
	never really completed. /dev/poll use on Linux is not recommended.

	See Poller\_devpoll (cc, h benchmarks ) for an example of how to use /dev/poll interchangeably with many other 
	readiness notification schemes. (Caution - the example is for Linux /dev/poll, might not work right on Solaris.)

	\item kqueue()
	
	This is the recommended poll replacement for FreeBSD (and, soon, NetBSD).
	See below. kqueue() can specify either edge triggering or level triggering.
	
	\end{enumerate}
\subsection{Serve many clients with each thread, and use nonblocking I/O and readiness change notification}

Readiness change notification (or edge-triggered readiness notification) means you give the kernel a file descriptor, and later, when that descriptor transitions from not ready to ready, the kernel notifies you somehow. It then assumes you know the file descriptor is ready, and will not send any more readiness notifications of that type for that file descriptor until you do something that causes the file descriptor to no longer be ready (e.g. until you receive the EWOULDBLOCK error on a send, recv, or accept call, or a send or recv transfers less than the requested number of bytes).

When you use readiness change notification, you must be prepared for spurious events, since one common implementation is to signal readiness whenever any packets are received, regardless of whether the file descriptor was already ready.

This is the opposite of "level-triggered" readiness notification. It's a bit less forgiving of programming mistakes, since if you miss just one event, the connection that event was for gets stuck forever. Nevertheless, I have found that edge-triggered readiness notification made programming nonblocking clients with OpenSSL easier, so it's worth trying.

[Banga, Mogul, Drusha '99] described this kind of scheme in 1999.

There are several APIs which let the application retrieve 'file descriptor became ready' notifications:
\begin{itemize}

	\item kqueue() This is the recommended edge-triggered poll replacement for FreeBSD (and, soon, NetBSD).
FreeBSD 4.3 and later, and NetBSD-current as of Oct 2002, support a generalized alternative to poll() called kqueue()/kevent(); it supports both edge-triggering and level-triggering. (See also Jonathan Lemon's page and his BSDCon 2000 paper on kqueue().)

Like /dev/poll, you allocate a listening object, but rather than opening the file /dev/poll, you call kqueue() to allocate one. To change the events you are listening for, or to get the list of current events, you call kevent() on the descriptor returned by kqueue(). It can listen not just for socket readiness, but also for plain file readiness, signals, and even for I/O completion.

Note: as of October 2000, the threading library on FreeBSD does not interact well with kqueue(); evidently, when kqueue() blocks, the entire process blocks, not just the calling thread.

See Poller\_kqueue (cc, h, benchmarks) for an example of how to use kqueue() interchangeably with many other readiness notification schemes.

Examples and libraries using kqueue():

\begin{itemize}
\item PyKQueue -- a Python binding for kqueue()
\item Ronald F. Guilmette's example echo server; see also his 28 Sept 2000 post on freebsd.questions.
\end{itemize}

\item epoll
This is the recommended edge-triggered poll replacement for the 2.6 Linux kernel.
On 11 July 2001, Davide Libenzi proposed an alternative to realtime signals; his patch provides what he now calls /dev/epoll www.xmailserver.org/linux-patches/nio-improve.html. This is just like the realtime signal readiness notification, but it coalesces redundant events, and has a more efficient scheme for bulk event retrieval.

Epoll was merged into the 2.5 kernel tree as of 2.5.46 after its interface was changed from a special file in /dev to a system call, sys\_epoll. A patch for the older version of epoll is available for the 2.4 kernel.

There was a lengthy debate about unifying epoll, aio, and other event sources on the linux-kernel mailing list around Halloween 2002. It may yet happen, but Davide is concentrating on firming up epoll in general first.

\item Polyakov's kevent (Linux 2.6+) News flash: On 9 Feb 2006, and again on 9 July 2006, Evgeniy Polyakov posted patches which seem to unify epoll and aio; his goal is to support network AIO. See:

\begin{itemize}
\item the LWN article about kevent
\item his July announcement
\item his kevent page
\item his naio page
\item some recent discussion
\end{itemize}

\item Drepper's New Network Interface (proposal for Linux 2.6+)
At OLS 2006, Ulrich Drepper proposed a new high-speed asynchronous networking API. See:

\begin{itemize}
\item his paper, "The Need for Asynchronous, Zero-Copy Network I/O"
\item his slides
\item LWN article from July 22
\end{itemize}

\item Realtime Signals
This is the recommended edge-triggered poll replacement for the 2.4 Linux kernel.
The 2.4 linux kernel can deliver socket readiness events via a particular realtime signal. Here's how to turn this behavior on:

\begin{verbatim}
/* Mask off SIGIO and the signal you want to use. */
sigemptyset(&sigset);
sigaddset(&sigset, signum);
sigaddset(&sigset, SIGIO);
sigprocmask(SIG_BLOCK, &m_sigset, NULL);
/* 
For each file descriptor, invoke F_SETOWN, F_SETSIG, and set O_ASYNC. 
*/
fcntl(fd, F_SETOWN, (int) getpid());
fcntl(fd, F_SETSIG, signum);
flags = fcntl(fd, F_GETFL);
flags |= O_NONBLOCK|O_ASYNC;
fcntl(fd, F_SETFL, flags);
\end{verbatim}

This sends that signal when a normal I/O function like read() or write() completes. To use this, write a normal poll() outer loop, and inside it, after you've handled all the fd's noticed by poll(), you loop calling sigwaitinfo().
If sigwaitinfo or sigtimedwait returns your realtime signal, siginfo.si\_fd and siginfo.si\_band give almost the same information as pollfd.fd and pollfd.revents would after a call to poll(), so you handle the i/o, and continue calling sigwaitinfo().

If sigwaitinfo returns a traditional SIGIO, the signal queue overflowed, so you flush the signal queue by temporarily changing the signal handler to SIG\_DFL, and break back to the outer poll() loop. 
See Poller\_sigio (cc, h) for an example of how to use rtsignals interchangeably with many other readiness notification schemes.

See Zach Brown's phhttpd for example code that uses this feature directly. (Or don't; phhttpd is a bit hard to figure out...)

[Provos, Lever, and Tweedie 2000] describes a recent benchmark of phhttpd using a variant of sigtimedwait(), sigtimedwait4(), that lets you retrieve multiple signals with one call. Interestingly, the chief benefit of sigtimedwait4() for them seemed to be it allowed the app to gauge system overload (so it could behave appropriately). (Note that poll() provides the same measure of system overload.)

\item Signal-per-fd
Chandra and Mosberger proposed a modification to the realtime signal approach called "signal-per-fd" which reduces or eliminates realtime signal queue overflow by coalescing redundant events. It doesn't outperform epoll, though. Their paper ( www.hpl.hp.com/techreports/2000/HPL-2000-174.html) compares performance of this scheme with select() and /dev/poll.

Vitaly Luban announced a patch implementing this scheme on 18 May 2001; his patch lives at www.luban.org/GPL/gpl.html. (Note: as of Sept 2001, there may still be stability problems with this patch under heavy load. dkftpbench at about 4500 users may be able to trigger an oops.)

See Poller\_sigfd (cc, h) for an example of how to use signal-per-fd interchangeably with many other readiness notification schemes.

\end{itemize}
\subsection{Serve many clients with each server thread, and use asynchronous I/O}

This has not yet become popular in Unix, probably because few operating systems support asynchronous I/O, also possibly because it (like nonblocking I/O) requires rethinking your application. Under standard Unix, asynchronous I/O is provided by the aio\_ interface (scroll down from that link to "Asynchronous input and output"), which associates a signal and value with each I/O operation. Signals and their values are queued and delivered efficiently to the user process. This is from the POSIX 1003.1b realtime extensions, and is also in the Single Unix Specification, version 2.

AIO is normally used with edge-triggered completion notification, i.e. a signal is queued when the operation is complete. (It can also be used with level triggered completion notification by calling aio\_suspend(), though I suspect few people do this.)

glibc 2.1 and later provide a generic implementation written for standards compliance rather than performance.

Ben LaHaise's implementation for Linux AIO was merged into the main Linux kernel as of 2.5.32. It doesn't use kernel threads, and has a very efficient underlying api, but (as of 2.6.0-test2) doesn't yet support sockets. (There is also an AIO patch for the 2.4 kernels, but the 2.5/2.6 implementation is somewhat different.) More info:

\begin{itemize}
\item The page "Kernel Asynchronous I/O (AIO) Support for Linux" which tries to tie together all info about the 2.6 kernel's implementation of AIO (posted 16 Sept 2003)
\item Round 3: aio vs /dev/epoll by Benjamin C.R. LaHaise (presented at 2002 OLS)
\item Asynchronous I/O Suport in Linux 2.5, by Bhattacharya, Pratt, Pulaverty, and Morgan, IBM; presented at OLS '2003
\item Design Notes on Asynchronous I/O (aio) for Linux by Suparna Bhattacharya -- compares Ben's AIO with SGI's KAIO and a few other AIO projects
\item Linux AIO home page - Ben's preliminary patches, mailing list, etc.
\item linux-aio mailing list archives
\item libaio-oracle - library implementing standard Posix AIO on top of libaio. First mentioned by Joel Becker on 18 Apr 2003.
\end{itemize}

Suparna also suggests having a look at the the DAFS API's approach to AIO.

Red Hat AS and Suse SLES both provide a high-performance implementation on the 2.4 kernel; it is related to, but not completely identical to, the 2.6 kernel implementation.

In February 2006, a new attempt is being made to provide network AIO; see the note above about Evgeniy Polyakov's kevent-based AIO.

In 1999, SGI implemented high-speed AIO for Linux. As of version 1.1, it's said to work well with both disk I/O and sockets. It seems to use kernel threads. It is still useful for people who can't wait for Ben's AIO to support sockets.

The O'Reilly book POSIX.4: Programming for the Real World is said to include a good introduction to aio.

A tutorial for the earlier, nonstandard, aio implementation on Solaris is online at Sunsite. It's probably worth a look, but keep in mind you'll need to mentally convert "aioread" to "aio\_read", etc.

Note that AIO doesn't provide a way to open files without blocking for disk I/O; if you care about the sleep caused by opening a disk file, Linus suggests you should simply do the open() in a different thread rather than wishing for an aio\_open() system call.

Under Windows, asynchronous I/O is associated with the terms "Overlapped I/O" and IOCP or "I/O Completion Port". Microsoft's IOCP combines techniques from the prior art like asynchronous I/O (like aio\_write) and queued completion notification (like when using the aio\_sigevent field with aio\_write) with a new idea of holding back some requests to try to keep the number of running threads associated with a single IOCP constant. For more information, see Inside I/O Completion Ports by Mark Russinovich at sysinternals.com, Jeffrey Richter's book "Programming Server-Side Applications for Microsoft Windows 2000" (Amazon, MSPress), U.S. patent \#06223207, or MSDN.

\subsection{Serve one client with each server thread}

... and let read() and write() block. Has the disadvantage of using a whole stack frame for each client, which costs memory. Many OS's also have trouble handling more than a few hundred threads. If each thread gets a 2MB stack (not an uncommon default value), you run out of *virtual memory* at (2\^30 / 2\^21) = 512 threads on a 32 bit machine with 1GB user-accessible VM (like, say, Linux as normally shipped on x86). You can work around this by giving each thread a smaller stack, but since most thread libraries don't allow growing thread stacks once created, doing this means designing your program to minimize stack use. You can also work around this by moving to a 64 bit processor.

The thread support in Linux, FreeBSD, and Solaris is improving, and 64 bit processors are just around the corner even for mainstream users. Perhaps in the not-too-distant future, those who prefer using one thread per client will be able to use that paradigm even for 10000 clients. Nevertheless, at the current time, if you actually want to support that many clients, you're probably better off using some other paradigm.

For an unabashedly pro-thread viewpoint, see Why Events Are A Bad Idea (for High-concurrency Servers) by von Behren, Condit, and Brewer, UCB, presented at HotOS IX. Anyone from the anti-thread camp care to point out a paper that rebuts this one? :-)

\subsubsection{LinuxThreads}

LinuxTheads is the name for the standard Linux thread library. It is integrated into glibc since glibc2.0, and is mostly Posix-compliant, but with less than stellar performance and signal support.

\subsubsection{NGPT: Next Generation Posix Threads for Linux}

NGPT is a project started by IBM to bring good Posix-compliant thread support to Linux. It's at stable version 2.2 now, and works well... but the NGPT team has announced that they are putting the NGPT codebase into support-only mode because they feel it's "the best way to support the community for the long term". The NGPT team will continue working to improve Linux thread support, but now focused on improving NPTL. (Kudos to the NGPT team for their good work and the graceful way they conceded to NPTL.)

\subsubsection{NPTL: Native Posix Thread Library for Linux}

NPTL is a project by Ulrich Drepper (the benevolent dict\^H\^H\^H\^Hmaintainer of glibc) and Ingo Molnar to bring world-class Posix threading support to Linux.

As of 5 October 2003, NPTL is now merged into the glibc cvs tree as an add-on directory (just like linuxthreads), so it will almost certainly be released along with the next release of glibc.

The first major distribution to include an early snapshot of NPTL was Red Hat 9. (This was a bit inconvenient for some users, but somebody had to break the ice...)

NPTL links:

\begin{itemize}
\item Mailing list for NPTL discussion
\item NPTL source code
\item Initial announcement for NPTL
\item Original whitepaper describing the goals for NPTL
\item Revised whitepaper describing the final design of NPTL
\item Ingo Molnar's first benchmark showing it could handle 10\^6 threads
\item Ulrich's benchmark comparing performance of LinuxThreads, NPTL, and IBM's NGPT. It seems to show NPTL is much faster than NGPT.
\end{itemize}

Here's my try at describing the history of NPTL (see also Jerry Cooperstein's article):

In March 2002, Bill Abt of the NGPT team, the glibc maintainer Ulrich Drepper, and others met to figure out what to do about LinuxThreads. One idea that came out of the meeting was to improve mutex performance; Rusty Russell et al subsequently implemented fast userspace mutexes (futexes)), which are now used by both NGPT and NPTL. Most of the attendees figured NGPT should be merged into glibc.

Ulrich Drepper, though, didn't like NGPT, and figured he could do better. (For those who have ever tried to contribute a patch to glibc, this may not come as a big surprise :-) Over the next few months, Ulrich Drepper, Ingo Molnar, and others contributed glibc and kernel changes that make up something called the Native Posix Threads Library (NPTL). NPTL uses all the kernel enhancements designed for NGPT, and takes advantage of a few new ones. Ingo Molnar described the kernel enhancements as follows:

\begin{quotation}
While NPTL uses the three kernel features introduced by NGPT: getpid() returns PID, CLONE\_THREAD and futexes; NPTL also uses (and relies on) a much wider set of new kernel features, developed as part of this project.
Some of the items NGPT introduced into the kernel around 2.5.8 got modified, cleaned up and extended, such as thread group handling (CLONE\_THREAD). [the\\ CLONE\_THREAD changes which impacted NGPT's compatibility got synced with the NGPT folks, to make sure NGPT does not break in any unacceptable way.]

The kernel features developed for and used by NPTL are described in the design whitepaper, http://people.redhat.com/drepper/nptl-design.pdf ...

A short list: TLS support, various clone extensions (CLONE\_SETTLS, \\CLONE\_SETTID, CLONE\_CLEARTID), POSIX thread-signal handling, sys\_exit() extension (release TID futex upon VM-release), the sys\_exit\_group() system-call, sys\_execve() enhancements and support for detached threads.

There was also work put into extending the PID space - eg. procfs crashed due to 64K PID assumptions, max\_pid, and pid allocation scalability work. Plus a number of performance-only improvements were done as well.

In essence the new features are a no-compromises approach to 1:1 threading - the kernel now helps in everything where it can improve threading, and we precisely do the minimally necessary set of context switches and kernel calls for every basic threading primitive.
\end{quotation}

One big difference between the two is that NPTL is a 1:1 threading model, whereas NGPT is an M:N threading model (see below). In spite of this, Ulrich's initial benchmarks seem to show that NPTL is indeed much faster than NGPT. (The NGPT team is looking forward to seeing Ulrich's benchmark code to verify the result.)

\subsubsection{FreeBSD threading support}

FreeBSD supports both LinuxThreads and a userspace threading library. Also, a M:N implementation called KSE was introduced in FreeBSD 5.0. For one overview, see \\www.unobvious.com/bsd/freebsd-threads.html.
On 25 Mar 2003, Jeff Roberson posted on freebsd-arch:

\begin{quotation}
... Thanks to the foundation provided by Julian, David Xu, Mini, Dan Eischen, and everyone else who has participated with KSE and libpthread development Mini and I have developed a 1:1 threading implementation. This code works in parallel with KSE and does not break it in any way. It actually helps bring M:N threading closer by testing out shared bits. ...
\end{quotation}

And in July 2006, Robert Watson proposed that the 1:1 threading implementation become the default in FreeBsd 7.x:

\begin{quotation}
I know this has been discussed in the past, but I figured with 7.x trundling forward, it was time to think about it again. In benchmarks for many common applications and scenarios, libthr demonstrates significantly better performance over libpthread... libthr is also implemented across a larger number of our platforms, and is already libpthread on several. The first recommendation we make to MySQL and other heavy thread users is "Switch to libthr", which is suggestive, also! ... So the strawman proposal is: make libthr the default threading library on 7.x.
\end{quotation}

\subsubsection{NetBSD threading support}

According to a note from Noriyuki Soda:

\begin{quotation}
Kernel supported M:N thread library based on the Scheduler Activations model is merged into NetBSD-current on Jan 18 2003.
\end{quotation}

For details, see An Implementation of Scheduler Activations on the NetBSD Operating System by Nathan J. Williams, Wasabi Systems, Inc., presented at FREENIX '02.

\subsubsection{Solaris threading support}

The thread support in Solaris is evolving... from Solaris 2 to Solaris 8, the default threading library used an M:N model, but Solaris 9 defaults to 1:1 model thread support. See Sun's multithreaded programming guide and Sun's note about Java and Solaris threading.

\subsubsection{Java threading support in JDK 1.3.x and earlier}

As is well known, Java up to JDK1.3.x did not support any method of handling network connections other than one thread per client. Volanomark is a good microbenchmark which measures throughput in messsages per second at various numbers of simultaneous connections. As of May 2003, JDK 1.3 implementations from various vendors are in fact able to handle ten thousand simultaneous connections -- albeit with significant performance degradation. See Table 4 for an idea of which JVMs can handle 10000 connections, and how performance suffers as the number of connections increases.

\subsubsection{Note: 1:1 threading vs. M:N threading}

There is a choice when implementing a threading library: you can either put all the threading support in the kernel (this is called the 1:1 threading model), or you can move a fair bit of it into userspace (this is called the M:N threading model). At one point, M:N was thought to be higher performance, but it's so complex that it's hard to get right, and most people are moving away from it.

\begin{itemize}
\item Why Ingo Molnar prefers 1:1 over M:N
\item Sun is moving to 1:1 threads
\item NGPT is an M:N threading library for Linux.
\item Although Ulrich Drepper planned to use M:N threads in the new glibc threading library, he has since switched to the 1:1 threading model.
\item MacOSX appears to use 1:1 threading.
\item FreeBSD and NetBSD appear to still believe in M:N threading... The lone holdouts? Looks like freebsd 7.0 might switch to 1:1 threading (see above), so perhaps M:N threading's believers have finally been proven wrong everywhere.
\end{itemize}

\subsection{Build the server code into the kernel}

Novell and Microsoft are both said to have done this at various times, at least one NFS implementation does this, khttpd does this for Linux and static web pages, and "TUX" (Threaded linUX webserver) is a blindingly fast and flexible kernel-space HTTP server by Ingo Molnar for Linux. Ingo's September 1, 2000 announcement says an alpha version of TUX can be downloaded from ftp://ftp.redhat.com/pub/redhat/tux, and explains how to join a mailing list for more info. 

The linux-kernel list has been discussing the pros and cons of this approach, and the consensus seems to be instead of moving web servers into the kernel, the kernel should have the smallest possible hooks added to improve web server performance. That way, other kinds of servers can benefit. See e.g. Zach Brown's remarks about userland vs. kernel http servers. It appears that the 2.4 linux kernel provides sufficient power to user programs, as the X15 server runs about as fast as Tux, but doesn't use any kernel modifications.

\section{Comments}

Richard Gooch has written a paper discussing I/O options.

In 2001, Tim Brecht and MMichal Ostrowski measured various strategies for simple select-based servers. Their data is worth a look.

In 2003, Tim Brecht posted source code for userver, a small web server put together from several servers written by Abhishek Chandra, David Mosberger, David Pariag, and Michal Ostrowski. It can use select(), poll(), epoll(), or sigio.

Back in March 1999, Dean Gaudet posted:

I keep getting asked "why don't you guys use a select/event based model like Zeus? It's clearly the fastest." ...

His reasons boiled down to "it's really hard, and the payoff isn't clear". Within a few months, though, it became clear that people were willing to work on it.

Mark Russinovich wrote an editorial and an article discussing I/O strategy issues in the 2.2 Linux kernel. Worth reading, even he seems misinformed on some points. In particular, he seems to think that Linux 2.2's asynchronous I/O (see F\_SETSIG above) doesn't notify the user process when data is ready, only when new connections arrive. This seems like a bizarre misunderstanding. See also comments on an earlier draft, Ingo Molnar's rebuttal of 30 April 1999, Russinovich's comments of 2 May 1999, a rebuttal from Alan Cox, and various posts to linux-kernel. I suspect he was trying to say that Linux doesn't support asynchronous disk I/O, which used to be true, but now that SGI has implemented KAIO, it's not so true anymore.

See these pages at sysinternals.com and MSDN for information on "completion ports", which he said were unique to NT; in a nutshell, win32's "overlapped I/O" turned out to be too low level to be convenient, and a "completion port" is a wrapper that provides a queue of completion events, plus scheduling magic that tries to keep the number of running threads constant by allowing more threads to pick up completion events if other threads that had picked up completion events from this port are sleeping (perhaps doing blocking I/O).

See also OS/400's support for I/O completion ports.

There was an interesting discussion on linux-kernel in September 1999 titled "> 15,000 Simultaneous Connections" (and the second week of the thread). Highlights:

\begin{itemize}
\item Ed Hall posted a few notes on his experiences; he's achieved >1000 connects/second on a UP P2/333 running Solaris. His code used a small pool of threads (1 or 2 per CPU) each managing a large number of clients using "an event-based model".
\item Mike Jagdis posted an analysis of poll/select overhead, and said "The current select/poll implementation can be improved significantly, especially in the blocking case, but the overhead will still increase with the number of descriptors because select/poll does not, and cannot, remember what descriptors are interesting. This would be easy to fix with a new API. Suggestions are welcome..."
\item Mike posted about his work on improving select() and poll().
\item Mike posted a bit about a possible API to replace poll()/select(): "How about a 'device like' API where you write 'pollfd like' structs, the 'device' listens for events and delivers 'pollfd like' structs representing them when you read it? ... "
\item Rogier Wolff suggested using "the API that the digital guys suggested", \\http://www.cs.rice.edu/\~gaurav/papers/usenix99.ps
\item Joerg Pommnitz pointed out that any new API along these lines should be able to wait for not just file descriptor events, but also signals and maybe SYSV-IPC. Our synchronization primitives should certainly be able to do what Win32's WaitForMultipleObjects can, at least.
\item Stephen Tweedie asserted that the combination of F\_SETSIG, queued realtime signals, and sigwaitinfo() was a superset of the API proposed in http://www.cs.rice.edu/\~gaurav/papers/usenix99.ps. He also mentions that you keep the signal blocked at all times if you're interested in performance; instead of the signal being delivered asynchronously, the process grabs the next one from the queue with sigwaitinfo().
\item Jayson Nordwick compared completion ports with the F\_SETSIG synchronous event model, and concluded they're pretty similar.
\item Alan Cox noted that an older rev of SCT's SIGIO patch is included in 2.3.18ac.
\item Jordan Mendelson posted some example code showing how to use F\_SETSIG.
\item Stephen C. Tweedie continued the comparison of completion ports and F\_SETSIG, and noted: "With a signal dequeuing mechanism, your application is going to get signals destined for various library components if libraries are using the same mechanism," but the library can set up its own signal handler, so this shouldn't affect the program (much).
\item Doug Royer noted that he'd gotten 100,000 connections on Solaris 2.6 while he was working on the Sun calendar server. Others chimed in with estimates of how much RAM that would require on Linux, and what bottlenecks would be hit.
\end{itemize}
Interesting reading!


\chapter{C10K 问题}

	现在的web服务器同时要处理上万个客户端，你不这样认为么？毕竟，现在互联网是一个强大的地方。
	
	计算机也同样强大。你可以花1200美元购买一台有1000MHz的CPU，2GB内存和1000Mbit/秒以太网卡的机器。让我们来看，有20000个客户端，每个客户端可以得到50Hz的CPU，100Kbytes的内存和50Kbits每秒的带宽。这不会消耗太多的时间为20000个客户端中的每一个客户端每一秒从磁盘读取4Kbytes的数据然后通过网络发送给她们。（顺便说一句，每个客户端花费0.08美元。一些操作系统费用中，每个客户端100美元的许可费用看起来有点重了！）因此，硬件已经不是瓶颈。
	
	1999年，一个最繁忙的ftp站点，cdrom.com，确实同时处理10000个客户端通过一个万兆以太网络。到了2001年，多个ISP服务商提供同样的速度，期望cdrom.com能够持续增长，以成为她们的大商业客户。
	
	这时，瘦客户端的计算模型似乎以某种形式回来了---在互联网上，服务器为成千上万的客户端同时提供服务。
	
	考虑到这一点，在如何配置操作系统和如何编写代码来支持成千上万的客户端，这里有一些札记。由于是我感兴趣的领域，这些讨论围绕类Unix操作系统，当然，Windows也会提及一点。	

\section*{相关的网站}

	在2003年十月，Felix von Leitner集中了一个优秀的网页并展示了关于网络的可扩展性，完成了比较不同网络系统调用和操作系统的基准。他的其中之一的结论就是Linux2.6内核完全击败了Linux2.4内核，同样，也有许多好的图表可以提供给操组系统开发者很好的思想源泉。（参看Slashdot的注解。看看一些人是怎样做后续基准程序来提高Felix的结果是非常有意思的。）

\section*{要先看的书}

	如果你还没有读过W. Richard Stevens的Unix网络编程：网络API：套接字联网（卷一），去读一遍吧。书中描述了很多关于编写高性能服务器的I/O策略和陷阱。书中甚至讨论'thundering herd'问题。如果你读完了，去看Jeff Darcy关于高性能服务器设计的札记。
（另一本Cal Henderson的构建可扩展的web站点（Building Scalable Web Sites）对那些用web服务器而不是写服务器的人更有帮助。）

\section*{I/O框架}
	目前有关于下面展示的技术的预先包装的库提供，这些库可以使你的代码不依赖于操作系统，使其更具可移植性。
	\begin{enumerate}
	
	\item ACE，一个重量级的C++ I/O框架，包含一些I/O策略的面向对象的实现和一些其他有用的东西。在具体实践中，他的Reactor模式是处理非阻塞I/O的一种面向对象处理方式，Proactor则是处理异步I/O的面向对象方式。
	\item ASIO是一个C++ I/O框架并且已经成为Boost库的一部分。它同ACE一样更新STL库。
	\item libevent是Niels Provos编写的一个轻量级的C I/O框架。它支持kqueue和select，并且很快就会支持poll和epoll。它仅有水平触发模式，我认为有好处也有坏处。Niels有一个不错的时间图处理一个作为连接数量的函数的事件。它显示kqueue和sys\_epoll是绝对的胜利者。
	\item 我自己的轻量级框架的尝试（很遗憾，没有持续更新）：
		\begin{enumerate}
		\item Poller是一个轻量级的C++ I/O框架，实现了使用任何你想要的依赖就绪API（poll，select，/dev/poll，kqueue或者sigio）的水平触发就绪API。它对于比较不同API的表现的基准很有用处。下面这个连接到Poller子类的文档展示了如何使用每一个就绪API。
		\item rn是一个轻量级的C I/O框架，是我在Poller之后的第二次尝试。它是LGPL（因此更容易在社区程序中使用），用C编写（因此易于在非C++的程序中使用。）它在一些社区项目中被使用。
		\end{enumerate}
	\item Matt Welsh在2000年四月份写了一篇关于在构建可扩展服务器是平衡工作线程和事件驱动技术的使用的论文。论文中涉及了Sandstorm I/O框架的一部分。
	\item Cory Nelson's Scale!库--一个Windows上的非同步套接字，文件和管道的I/O库。
	\end{enumerate}
	
\section*{I/O策略}

网络软件设计人员有很多选择。下面就是一些：
	\begin{enumerate}
	
		\item 在一个单独的线程中是否处理多个I/O调用并且怎样处理。
		
			\begin{enumerate}
				\item 绝对。始终使用阻塞/同步调用，并且可能的话，使用多线程或多进程实现并发。
				\item 使用非阻塞调用（例如：write()一个套接字时设置为O\_NONBLOCK）来启动I/O，并且使用就绪通知机制（例如：poll()或者/dev/poll）来获得在那个通道上什么时候可以开始下一个I/O。
				\item 使用异步调用（例如：aio\_write()）来开始I/O，并且使用完全通知机制（例如：信号或者完成端口）来获得I/O完成的时间。同时适用于网络和磁盘I/O。
			\end{enumerate}
		\item 怎样控制为每个客户端提供服务的代码。
			\begin{enumerate}
				\item 每个客户端一个进程（典型的Unix解决方案，从1980年开始使用）
				\item 一个操作系统层面的线程处理很多客户端。每个客户端的控制者为：
				\begin{enumerate}
					\item  一个用户层面的线程（例如：GNU状态线程，经典的Java绿色线程）
					\item  一个状态机（有点深奥，但是在一些地方很流行。我的最爱。）。
					\item  一个续集（有点深奥，但是同样在一些地方很流行）。
				\end{enumerate}
				\item 每个客户端一个操作系统层面的线程（例如：经典的Java本地线程）。
				\item 每个活动的客户端一个操作系统层面的线程（例如：Tomcat的apache前端，NT完成端口，线程池）。
			\end{enumerate}
		\item 是否使用标准的操作系统服务，或者把一些代码放到内核中。（例如：在一个第三方驱动，内核模块或者虚拟设备驱动（VxD））
	\end{enumerate}
	
	
下面的五种则和很流行：
	\begin{itemize}
		\item 一个线程处理多个客户端，使用非阻塞I/O和水平触发就绪通知机制。
		\item 一个线程处理多个客户端，使用非阻塞I/O和就绪状态改变通知机制。
		\item 多个线程处理多个客户端，使用异步I/O。
		\item 每个线程处理一个客户端，使用阻塞I/O。
		\item 把服务器代码加到内核中。
	\end{itemize}
	
\subsection*{一个线程处理多个客户端，使用非阻塞I/O和水平触发就绪通知机制。}

	将所有网络句柄设置成非阻塞模式，使用select()或者poll()来获知那个网络句柄有数据可读。这个是经典的传统模式。使用这种方案，操作系统内核告诉你那个文件描述符已经就绪，并且告诉你从最后一次内核告诉你这个事件是否你已经做了一些操作在那个文件描述符上。（名词“水平触发”来自计算机硬件设计。和“边际触发”相对。Jonathon Lemon在他的关于kqueue()的BSDCON 2000论文中介绍了这些。）
	
	注意：记住内核所发出的就绪通知仅仅是一个示意是很重要的。在你尝试从文件描述符读数据的时候，它可能压根就没有就绪。这就是为什么当使用就绪通知机制时使用非阻塞模式非常重要。
	这种方法的一个重要瓶颈是当请求的页此时不在核心上，read()或者sendfile()磁盘阻塞。将磁盘文件描述符设置成非阻塞没有什么作用。使用内存映射磁盘文件也一样。服务器第一次需要磁盘I/O时，它阻塞了，所有的客户端必须等待，这样原生非线程效率就被浪费了。
	
	这就是异步I/O要去解决的问题。但是在没有异步I/O的系统上，工作线程或进程进行磁盘I/O是依然会遇到这个瓶颈。一个解决方案是使用内存映射文件，如果mincore()标明需要I/O，调用一个工作线程去处理I/O，同时急促处理网络传输。Jef Poskanzer指出Pai, Druschel, 和Zwaenepoel的1999 Flash 网络服务器使用了这个方案。在Usenix '99他们有一个关于这个的讨论。似乎mincore()在一些BSD驱动的Unix，比如FreeBSD和Solaris上已经提供，但mincore()不是单一Unix标注的一部分。多亏Chuck Lever，mincore()作为Linux2.3.51内核的一部分提供。
	
	但是2003年11月在freebsd黑客列表中，Vivek Pei et al上报了一个不错的成果，他们利用系统剖析工具剖析它们的Flash Web服务器，然后再攻击其瓶颈。mincore是他们发现的其中之一的瓶颈（试想对所有情况那并不都是一个好办法）。另一个则是sendfile阻塞在磁盘访问上的事实。他们通过引进一个修改版的sendfile()来提高性能，这个修改版的sendfile()在当所需要获取的磁盘页不在核心上时，返回类似于EWOULDBLOCK的信息。（不能确定你如何告诉用户那个也现在在核心中了。。。在我看来，这里真正需要的是aio\_sendfile()。）他们优化的结果是在一个1GHz/1GB FreeBSD盒子中，获得了SpecWeb99的800分。这要优于spec.org上其他任何在文件上的方案。

对于一个单线程，有多种方法可以获知在一个非阻塞的套接字集合中，那个套接字已经有I/O就绪：

	\begin{enumerate}
	
	\item 传统的select().
	
	不幸的是，select()的FD\_SETSIZE有限制。这个限制被编译到标准库和用户程序中。（一些版本的C库可以让你在用户程序编译期间提高这个限制。）请查看Poller\_select(cc,h)关于如何使用select()和其他就绪通知方案互换的例子。
	
	\item 传统的poll()
	
	没有关于poll()所能处理的文件描述符的数量的硬编码的限制，但是由于在某一时间上，大多数的文件描述符处于等待状态并且扫描上千的文件描述符需要花费很多时间，当描述符的数量上千之后，它开始变慢。一些操作系统通过使用一些技术，比如轮询示意来提高poll()的速度。轮询示意在1999年被Niels Provos为Linux实现并成为标准。请查看Poller\_poll(cc,h)关于如何使用poll()和其他就绪通知方案互换的例子。
	
	\item /dev/poll
	
	这个是Solaris建议的poll的替代品。
	
	/dev/poll背后的思想是充分利用poll()经常被以相同的参数调用很多次的事实。通过/dev/poll，你得到一个打开的句柄。通过向这个句柄写数据，你只需要告诉操作系统一次你对什么文件感兴趣。之后，你只需要从那个句柄读一个当前就绪的文件描述符的集合。
	
	/dev/poll在Solaris7中悄然出现（查看106541补丁）。但是在Solaris8中才第一次公开出现。根据Sun的说法，在有750哥客户端时，/dev/poll比poll()有10%的效率提升。
	
	/dev/poll的不同实现都在Linux上试验过，但是没有一个性能可以epoll相当，并且没有一个实现真正完成。/dev/poll不建议在Linux上使用。
	
	请查看Poller\_devpoll(cc, h 基准)关于如何使用/dev/poll和其他就绪通知方案互换的例子。（注意：这个例子是在Linux的/dev/poll，可能在Solaris下不能正确工作）。
	\item kqueue()
	
	这个是FreeBSD建议的poll的替代品。（并且，也是NetBSD的）。下面指出，kqueue()可以设置成边际触发或者水平触发。
	
	\end{enumerate}
	
\subsection*{每一个线程处理多个客户端，使用非阻塞I/O和就绪通知机制。}

	就绪通知机制（或者边际触发就绪通知机制）表示你给内核一个文件描述符，之后，当这个描述符从没有就绪转换成就绪，内核在某个时候通知你。然后，操作系统内核就假设你已经知道那个文件描述符已经就绪。内核将不再发送那种类型的关于那个文件描述符的更多的就绪通知给你，直到你做了一些事情使文件描述符编程不就绪。（例如，直到你接受到一个EWOULDBLOCK错误在send，recv或者accept调用上，或者一次send或者recv传送的数据小于需要传送的字节数。）
	
	当你使用就绪改变通知机制时，你必须为假事件做好准备。因为一个通常的实现是当任何接受到数据包时就发送就绪信号，不管这个文件描述符是否就绪。
	
	这是“水平触发”就绪通知机制的对立面。由于你错过了仅仅一个事件，那么这个事件所对应的连接将永远处于等待状态，这是一个微小的编程错误疏忽。尽管如此，我发现边际触发就绪通知机制是使用OpenSSL的非阻塞客户端编程变得容易，因此，这值得去尝试。
	
	[Banga, Mogul, Drusha '99]在1999年描述了这种方案。
	
	有几种API可以是程序获得“文件描述符就绪”的通知：
	\begin{enumerate}
	
	\item kqueue()
	
	这是FreeBSD推荐的边际触发poll的替代品。（同样，很快也是NetBSD的）。
	
	FreeBSD4.3以及后续版本和NetBSD 2002年10月份的当前版本支持一个通用的poll()的候选者叫kqueue()/kevents()。支持边际触发和水平触发。（参见Jonathan Lemon's的主页和他的关于kqueue()的BSDCon 2000 论文。）
	
	和/dev/poll类似，你分配一个监听对象，但是不是通过打开文件/dev/poll，而是调用kqueue()去分配一个。为了改变你正在监听的事件，或者得到当前事件的列表，你在kqueue()返回的描述符上调用kevent()。它不仅可以监听套接字的就绪，还可以监听普通的文件描述符，信号，甚至I/O完成。
	
	注意：在2000年10月，FreeBSD的线程库不能很好的同kqueue()交互。显然，当 kqueue() 阻塞，整个进程阻塞，而不是调用的线程。
	
	参见Poller\_kqueue（cc， h，基准）关于如何使用kqueue()和其他就绪通知机制互换的例子。
	
	使用kqueue()的例子和库：
		\begin{enumerate}
			\item PyKQueue--一个Python的kqueue()包转。
			\item Ronald F. Guilmette的echo服务器的例子。参见他的2000年9月28日在\\freebsd.questions 上的帖子。
		\end{enumerate}
	\item epoll
	
	这是Linux 2.6内核推荐的边际触发poll的替代品。
	
	在2001年7月11日，Davide Libenzi建议了一个实时信号的候选者。他的补丁提供了他所谓的/dev/epoll www.xmailserver.org/linux-patches/nio-improve.html。这和实时信号就绪通知机制类似，但是他能合并冗余事件，并且有更高效的对付大批事件获得的机制。
	
	Epoll在他的接口从一个特殊的/dev中的文件改变成校内他调用，sys\_epoll之后，以2.5.46合并到2.5内核树中。另外有为2.4内核提供的旧版本的epoll的补丁。
	
	2002年在linux内核邮件列表中，围绕Halloween有一个关于同一epoll，aio和其他别的事件资源的长时间的讨论。讨论也许不会在发生，但是Davide一直努力坚定epoll为通常情况下的首选。

	\item Polyakov的kevent(Linux 2.6或更高版本)的新闻：在2006年2月9日和7月6日，Evgeniy Polyakov提交了他的补丁。这个补丁看上去像是epoll和异步io的统一。它的目标是支持网络异步IO。参见：
	\begin{itemize}
		\item 关于kevent的LWN报告
		\item 他的kevent主页
		\item 他的naio主页
		\item 一些最近的讨论。
	\end{itemize}
	
	\item Drepper的新网络接口（Linux2.6内核的方案）

		在2006年的OLS，Ulrich Drepper建议了一个新的高速异步网络API。参见：
	\begin{itemize}
		\item 他的论文。《异步零拷贝网络I/O的需求》
		\item 他的展示。
		\item 7月22日的LWN文稿。
	\end{itemize}

	\item 实时信号

		这个建议用来替换Linux2.4内核的边际触发poll。

		linux2.4内核可以通过一个特殊的实时信号传递套接字就绪事件。下面是怎样打开这个特性：

		\begin{verbatim}
/* 屏蔽SIGIO信号和你想使用的信号 */
sigemptyset(&sigset);
sigaddset(&sigset, signum);
sigaddset(&sigset, SIGIO);
sigprocmask(SIG_BLOCK, &m_sigset, NULL);

/* 对于每一个文件描述符，设置F_SETOWN，F_SETSIG和O_ASYNC. */
fcntl(fd, F_SETOWN, (int) getpid());
fcntl(fd, F_SETSIG, signum);
flags = fcntl(fd, F_GETFL);
flags |= O_NONBLOCK|O_ASYNC;
fcntl(fd, F_SETFL, flags);
		\end{verbatim}
		
		当一个普通的I/O函数，如read()或者write()，完成时，这个将发送一个信号。
为了使用这个特性，在外循环中写一个普通的poll()函数，在循环里面，在你处理完所有poll()函数监测到的有事件的描述符之后，你循环调用sigwaitinfo()函数。

		如果sigwaitinfo或者sigtimedwait给你返回了一个实时信号，那么 siginfo.si\_fd 和 siginfo.si\_band 和poll()被调用之后的pollfd.fd和pollfd.revents有着相同的信息。因此，你可以处理I/O事件，然后继续调用sigwaitinfo()。

		参见Poller\_sigio(cc, h)关于如何使用实时信号和其他就绪通知机制的互换。

		参见关于如何直接使用这个特性的Zach Brown的phhttpd的代码。

		[Provos， Lever和Tweedie 2000]描述了最近的phhttpd使用不同的sigtimedwait()，sigtimedwait4()的基准，使你可以在一个调用中获得多个信号。另外，sigtimedwait4()函数的一点好处似乎可以允许程序测量系统的负载（那么，程序就可以有合适的行为）。（注意，poll()提供了同样的测量系统负载的功能。）

	\item  每个描述符一个信号

		Chandra和Mosberger建议了一个名叫“Signal-per-fd”的对实时信号实现的修改。这个修改可以通过消除冗余事件来减少或者消除实时信号队列的溢出。但是，它没有epoll的效率高。他们的论文比较了这个结构和select以及/dev/poll的效率。

		Vitaly Luban在2001年5月18日宣布一个这个结构的补丁实现。他的补丁在\\www.luban.org/GPL/gpl.html。（住：在2001年9月，这个补丁在高负载的时候依然存在这稳定性问题。dkftpbench在大约4500个用户的时候可能触发一个oops，也就是正确的行为却输出了一个错误日志。）
		
		参见Poller\_sigfd(cc,h)关于如何使用signal-per-fd和其他就绪通知机制交互。
	\end{enumerate}
 
\subsection*{每个线程处理多个用户，使用异步I/O}
		
	可能由于很少的系统支持异步I/O，也可能是因为这需要（如同非阻塞I/O一样）你重新构思你的应用，异步I/O还没有在Unix系统上流行起来。在标准Unix上，异步I/O通过aio\_ 接口提供（看下面的“异步输入输出”的链接），这个接口将一个信号和信号值与每一个I/O操作关联。信号和他们的值被排队并高效的传递个用户进程。这个来自POSIX 1003.1b实时扩展和单一Unix标准的第二版。

	异步I/O通常和边际触发完成机制一起使用，例如，一个信号被存入队列中直到操作完成。（通过调用aio\_suspend()，异步I/O也可以和水平触发完成机制一起使用，但是我很少看到有人这么做。）

	glibc2.1以及后续版本提供了一个通用的实现，这个实现是为了遵守标准而不是效率。

	Ben LaHaise的Linux异步I/O实现被合并到linux内核主分支中，最为2.5.32版本。它不使用内核线程，并且有一个非常高效的底层api，但是（例如2.6.0-test2）还不支持套接字。（有关于2.4内核的补丁，但和2.5/2.6的实现有所不同。）更多信息：

	\begin{itemize}
		\item 页面“关于Linux的内核异步I/O支持”尝试去将所有的关于2.6内核中异步I/O实现的信息组织在一起。（2003年9月16日发布。）
		\item 第三论：异步I/O vs /dev/epoll。Benjamin C.R. LaHaise（在2002年的OLS上发表）
		\item Linux2.5内核支持的异步I/O，Bhattachary，Pratt，Pulaverty和Morgan著，IBM ; 在 2003年的OLS发表。
		\item Suparna Bhattachaya的Linux下的异步I/O设计手记。比较了Ben的异步 I/O 和 SGI 的KAIO以及一些其他的异步I/O项目。
		\item Linux异步I/O主页。
		\item linux-aio邮件列表
		\item libaio-oracle;在libaio之上实现了标准Posix异步I/O的库。在2003年9月18日被Joel Becker第一次提及。
	\end{itemize}

	Suparna还建议去看看DAFS的关于AIO的API的实现。

	Red Hat AS和Suse SLES都提供了一个在2.4内核上的高效的实现。它和2.6内核的实现有关系但不完全相似。

	在2006年2月，一个新的尝试提供网络异步I/O。参见上面的关于Evgeniy Polyakov的基于kevent的AIO。

	在1999年，SGI为Linux实现了一个高速的AIO。在1.1版本的时候，据说可以同时在磁盘I/O和套接字上很好的工作。它好像使用了内核线程。这对那些等不及Ben的AIO支持套接字的人们来说依然很有用。

	O'Reilly的书POSIX.4：真实世界的编程包括了一个很好的关于aio的介绍。

	Sun的网站上包含一个关于早期的非标准aio在Solaris上的实现的教程。值得一看。但是记住你要在"aioread"和"aio\_read"之间转换。

	注意，AIO没有提供一个在没有对z磁盘I/O阻塞情况下打开文件的途径。如果你关心关于打开磁盘文件时引起的睡眠，Linus建议你在不同的线程中调用open()函数，而不是希望一个aio\_open()系统调用。

	在Windows下，异步IO和“重叠I/O”，IOCP或者“IO完成端口”有联系。微软的IOCP将早先的艺术例如异步I/O（比如io\_write）和队列完成通知机制（如将\\aio\_sigevent 值和 aio\_write 一起使用）技术组合起来，并增加一个阻止一些请求以试图保持同一个单一的IOCP常量相关连的运行线程的数量的新想法。更多的信息参见Mark Russinovich在sysinternals.com上的文章《深入I/O完成端口》，Jeffrey Richter的书《编写Microsoft Windows 2000服务器端程序》， 美国专利号：\#06223207，或者MSDN。



\subsection*{每个客户端一个线程处理}
	
	...让read()和write()阻塞。由于有每一个客户端使用一个完整的栈框的缺点，这个方法浪费内存。很多操作系统在同时处理几百个线程是有一些麻烦。如果一个线程需要2MB的栈（这时通产的默认值），在运行了（2\^30 / 2\^21）512个线程时，你将耗尽一个32位机器的1GB用户可使用的虚拟内存（例如，x86上运行的linux）。你可以通过给每个线程分配更小的栈来克服这个问题，但是由于大多数线程库不允许在创建线程后增加线程栈，这样做意味着你要设计好自己的程序来减少栈的使用。你也可以通过移植到一个64位的机器来克服这个问题。

	Linux，FreeBSD和Solaris对线程的支持不断增强。同时64位处理器将很快称为主流用户的选择。也许在不久的将来，那些喜欢使用每个客户端一个线程的开发者将能够使用这个设计为10000个客户端提供服务。但是，就目前而言，如果你真的想支持那么多用户，你最好使用别的设计。

	对于一些支持线程的观点，参见Behren，Condit和Brewer的《为什么时间不是好东西（对于高并发服务器）》，UCB，在HotOs IX发表。有没有发对线程的人发表一篇文章来驳斥这个观点？

\subsubsection*{LinuxThreads}
	LinuxThreads是标准Linux线程库的名称。从glibc2.0开始，它被集成到glibc中。它几乎是Posix兼容的，但是缺少效率和信号支持。

\subsubsection*{NGPT：Linux下一代线程}
	NGPT是IBM发起的旨在为Linux提供一个好的Posix兼容的线程支持。它现在的稳定版本是2.2，并且运行的很好。。。但是NGPT开发团队已经宣布他们将NGPT的代码库设置为仅支持不开发的模式。因为他们感觉这是“最好的长时间支持社区的办法”。NGPT开发团队将继续改进Linux的线程支持，但是现在他们的重点是改进NPTL。（荣誉属于NGPT团队，为他们优秀的工作和他们考虑NPTL的优雅的方法。）

\subsubsection*{NPTL：Linux原生Posix线程库}
	NPTL是Ulrich Drepper（和蔼的glibc的维护者）和Ingo Molnar发起的项目，旨在为Linux带来世界级的Posix线程支持。

	在2003年10月5日，NPTL被合并到glibc的版本控制树中，最为一个附加的目录（就像linuxthreads一样），因此，它几乎确定将在gblic的下一个发布版中发布。

	Red Hat9是第一个包含NPTL早期快照的主流发行版。（这也许对一些用户不太方便，但是必须有人能够破冰。。。）

	NPTL的链接：
	\begin{itemize}
		\item NPTL讨论邮件列表
		\item NPTL源码
		\item NPTL的初始声明
		\item 描述NPTL最终设计的原始白皮书
		\item 描述NPTL最红设计的修订白皮书
		\item Ingo Molnar的第一个测试标准现实NPTL可以处理10\^6个线程
		\item Ulrich的比较LinuxThreads，NPTL和IBM的NGPT的测试基准。这个似乎显示 NPTL 比 NGPT快多了。
	\end{itemize}

	下面是我关于NPTL历史的一些描述（同时参见Jerry Cooperstein的文章）：

	在2002年5月，NGPT的成员Bill Abt，glibc的维护者Ulrich Drepper和一些其他人见面讨论要对LinuxThreads做些什么。来自那次见面的一个想法是提高互斥锁的效率。 Rusty Russell et al后来实现了快速用户空间互斥锁（futexes），并且现在正被NGPT和NPTL使用。大部分的参与者认为NGPT应该合并到glibc中。

	然而Ulrich Drepper不喜欢NGPT，并且他指出他可以做的更好。（对那些曾经给\\glibc贡献过补丁的人来说，这也许不是什么吃惊的事。）在下面的几个月中，Ulrich Drepper，Ingo Molnar和其他一些人贡献了一些对glibc和内核的修改，被称作原生Posix线程库（NPTL）。NPTL使用了内核为NGPT设计的增强功能，并且利用了一些新的功能。Ingo Molnar如下的描述了这些内核增强功能：

	\begin{quotation}
		当NPTL使用了NGPT引入的三个内核特性：getpid()返回PID，\\ CLONE\_THREAD 和 futexes，NPTL也使用（并且依赖）了一些更宽泛的内核特性，这些特性作为这个项目的一部分。

		一些NGPT在2.5.8时引入的内核特性被修改，清除和扩展，例如线程组处理（CLONE\_THREAD）。(CLONE\_THREAD的修改影响了NGTP的和其他NGPT亲戚同步的兼容性，为了保证在任何不可接受的情况下NGPT都不会中断。)
		
		为NPTL设计并被NPTL使用而开发的内核特性在设计白皮书中描述。http://people.redhat.com/drepper/ntpl-design.pdf...

		一个简短列表：TLS支持，多种克隆扩展（CLONE\_SETLS, CLOSE\_SETTID, CLONE\_CLEARTID），POSIX线程信号处理， sys\_exit()扩展（在VM发布版上发布了TID futex），sys\_exit\_group()系统调用，sys\_execve()加强和分离线程的支持。

		还有关于扩展PID空间的工作被做，例如，由于64K PID的假设，max\_pid和pid分配扩展性工作，procfs会崩溃。还有一些仅性能方面的提高的工作也被做了。

		本质上，新的特性是实现1:1线程的非妥协处理。现在内核可以在任何提高线程能力的地方有所版主，并且我们只需要做上下文切换和基本线程原函数的内核调用的很小的一部分。
	\end{quotation}

	两者之间最大的却别在于NPTL是1:1的线程模型，而NGPT是M:N的线程模型（参见下面）。尽管如此，Ulrich的初始基准似乎显示NPTL确实毕NGPT快多了。（NGPT团队期待看打Ulrich的基准代码来修改结果。）

\subsubsection*{FreeBSd的线程支持}
	
	FreeBSD支持LinuxThreads和一个用户空间线程库。另外，一个叫做KSE的M:N的实现在FreeBSD5.0时被引入。参见www.unobvious.com/bsd/freebsd-threads.html

	在2003年5月25日，Jeff Roberson在freebsd-arch发了一个贴：
	\begin{quotation}
		...幸亏Julian, David Xu, Mini, DanEischen和所有其他投身KSE和libpthread开发的人建立的基础，Mini和我开发出了一个1:1的线程实现。这个实现和KSE并行运行宾切不会打断KSE。它实际上通过测试共享位，帮助我们更靠近了M:N线程模型。
	\end{quotation}

	在2006年7月，Robert Watson宣布在FreeBSD7.x中，1:1线程模型实现成为默认的：

	\begin{quotation}
		我知道这个在以前已经讨论过了，但是由于7.x的不断改进，我认为现在该再次考虑它了。在很多常规应用和情景的基准中，libthr显然毕libpthread有更好的性能...libthr实现了跨很多平台，并且包含那些libpthread已经支持的一些平台。第一个我们为MySQL和其他大量线程用户的建议是“改用libthr”，libthr也是推荐的!...因此strawman的建议是：是libthr成为7.x的默认线程库。
	\end{quotation}

\subsubsection*{NetBSD的线程支持}
		根据Noriyuki Soda的手记：
		
		在2003年7月18日，基于调度激活模型的支持M:N线程库的内核被合并到BetBSD当前版本中。

		更多细节参见Nathan J. Williams, Wasabi Systes, Inc.在FRENIX上发表的的NetBSD操作系统上调度激活的一个实现。
\subsubsection*{Solaris的线程支持}
	
	Solaris的线程支持在不断的演变。从Solaris2到Solaris8，默认的线程库是M:N模型，但是在Solaris9默认的线程库是1:1模型。参见Sun的多线程编程手册和Sun的关于Java和Solaris线程的手册。

\subsubsection*{在JDK1.3.x及后续版本中Java的线程支持}

	众所周知，Java直到JKD1.3.x都没有提供任何处理网络连接的方法，除了一个客户端一个线程。Volanomark是一个不错的微型测试程序，可以用来测量在某个时候不同数目的网络连接时每秒钟的信息吞吐量。在2003.5, JDK 1.3的实现实际上可以同时处理10000个连接，但是性能却严重下降了。从Table 4 可以看出JVMs可以处理10000个连接，但是随着连接数目的增长性能也逐步下降。

\subsubsection{注：1:1模型 vs M:N模型}

	在实现线程的时候，你有一个选择：你可以把所有线程的支持都放入内核中（这被成为1:1模型）或者你可以把一部分的线程支持移到用户空间中（这被成为M:N模型）。有一个观点认为，M:N模型有更高的性能，但是它太过复杂以致很难保证正确，并且大多数人正在远离他。
	\begin{itemize}
		\item 为什么Ingo Molnar更喜欢1:1,而不是M:N
		\item Sun正在转移到1：1模型上。
		\item NGPT是一个Linux下的M:N模型线程库。
		\item 尽管Ulrich Drepper计划在新的glibc线程库中使用M:N模型，但是他曾经转移到1:1模型过。
		\item MaxOSX看来使用1:1模型。
		\item FreeBSd和NetBSD似乎依然坚守M:N模型...最长的坚守者？好像freebsd7.0也许转移到1:1线程模型（参见上文），所以也学M:N线程模型的坚守者被证明一无是处。
	\end{itemize}
\subsection*{将服务器代码嵌入内核}

	Novell和微软在不同的时间都宣称已经实现了这个。至少，一个NFS的实现是这样的，khttpd是linux下的静态网页服务器，"TUX"(Threaded linUX webserver)是Ingo Molnar为Linux实现的相当快并且可扩展的内核空间HTTP服务器。Ingo的2000年9月1日的声明说一个alpha版本的TUX可以在ftp://ftp.redhat.com/pub/redhat/tux上下载，并且说明怎样加入一个邮件列表以获得更多的信息。

	linux-kernel邮件列表已经一直在讨论这个实现的优缺点，讨论的共识似乎是内核因改提供最小的可能的hooks的增加以提高web服务器的性能，而不是将web服务器移到内核中。这种方法也可使其他服务器受益。参见Zach Brown的关于用户空间vs内核http服务器的备注。linux2.4内核提供了有效的功能给用户程序，比如X15服务器运行的和Tux一样快，但没有对内核进行任何修改。

\section*{注解}
	Richard Gooch写了一篇讨论I/O选项的论文。
	
	在2001年，Tim Brecht和MMichal Ostrowski测量了各种策略下的简单基于select服务器的性能。他们的数据值得一看。

	在2003年，Tim Brecht发布了userver的源代码。这是一个下的web服务器，集合了Abhishek Chandra，David Mosberger，David Pariag和Michal Ostrowski所写的多个服务器。它可以使用select()，poll()，epoll()或者sigio。

	回到1999年五月，Dean Gaudet发帖说：

	我不断地被问道“为什么你们这些人不使用基于select/event的模型，就像Zeus？很明显那是最快的。”...

	他的理由归结为“很困难，并且回报不明显”。但是几个月后，越来越明显，人们喜欢使用它。

	Mark Russinovich写了一篇社论和论文讨论关于Linux2.2内核的I/O策略。值得一读，虽然他似乎乎略了一些观点。实际上，他似乎认为Linux2.2内核的异步I/O（参见上文F\_SETSIG）不会在数据准备就绪时通知用户进程，而是仅仅在新连接到达时通知。这看起来是一个异乎寻常的误解。参见一个早期草案的注解，Ingo Molnar在1999年4月30日的反驳，Russinovich在1999年5月2日的注解，Alan Cox的反驳和linux-kernel的帖子。我猜测他试图在说Linux不支持异步磁盘I/O，这个曾经确实是对的。但是现在，SGI已经实现了KAIO，这个不在是对的了。

	参见这些关于“完成端口”的sysinternals.com和MSDN的网页。他认为完成端口是NT特有的。总之，win32的“覆盖I/O”过于底层而不方便使用，并且一个“完成端口”是一个包装，提供了完成事件的队列，外加通过当别的已经从这个端口获得完成事件的线程处于睡眠状态时（可能在做阻塞I/O）， 允许更多更多的线程获得完成事件来保持运行状态的线程数量保持不变。
	
	参见OS/400的对完成端口的支持。

	1999年9月，在linux-kernel上有一个内部的讨论，“>15000并发连接”（线程的第二个星期）。主要包括：

	\begin{itemize}
		\item Ed Hall发布了一些关于他的经验的手记。他的在一个运行Solaris的HP P2/333上实现>1000 个连接/秒的方案。他的代码使用了一个小的线程池（1或2个每个CPU）每个线程处理大量客户端使用“一个基于事件的线程”。
		\item Mike Jagdis发布了一个关于poll/select开销的分析，他说“当前的select/poll实现可以被显著的提高，尤其是在阻塞的情况下。但是开销依然会随着描述符数量的增加而增加，这是因为select/poll没有，也不可能记住哪些描述符可能有事件。这个很容易通过一个新的API来修正。欢迎提建议...”
		\item Mike发布了他的关于提高select()和poll()性能的工作。
		\item Mike发布了一点关于一个可能替换select()/poll()的API：“在一写‘类pollfd’结构体的地方，一个‘类设备’的API如何，这个‘设备’监听事件并且在你读它的时候，它传送‘类pollfd’结构体来表示事件？...”
		\item Rogier Wolff建议使用“数字开发人员建议的API”，http://www.cs.rice.edu/\~gaurav/papers/usenix99.ps
		\item Joerg Pommnitz指出，这个讨论中的任何新的API必须不仅仅能构等待文件描述符的事件，还要能够等待信号和也许SYSV-IPC。至少，我们的同步原语能够确切的完成Win32的WaitForMultipleObjects能够完成的任务。
		\item Stephen Tweedie断言，F\_SETSIG，实时信号队列和sigwaitinfo()的组合是http://www.cs.rice.edu/\~gaurav/papers/usenix99.ps所提到的API的超集。他还提到，如果你很在意效率，那么一致保持信号阻塞。否则，进程从sigwaitinfo()的队列中抓取下一个信号，而不是异步的传送信号。
		\item Jayson Nordwick比较了完成端口和F\_SETSIG同步事件模型，得出的结论是他们非常的相似。
		\item Alax Cox指出，一个旧的SCT的SIGIO的rev补丁包含在2.3.18ac中。
		\item Jordan Mendelson发布了一些如何使用F\_SETSIG的示例代码。
		\item Stephen C. Tweedie继续比较了完成端口和F\_SETSIG，指出“当使用一个信号队列机制时，如果正在使用的库也使用了相同的机制，那么，你的程序将获得那些本来属于各种库组件的信号。”但是，库可以设置自己的信号处理函数，因此这个不会（过多）的影响程序。
		\item Doug Royer指出，当他在Sun日期服务器上工作的时候，他获得了100000个连接在Solaris2.6上。其他的关于如何估计在Linux需要多少RAM和将会遇到什么瓶颈的文章。
	\end{itemize}
	
	有趣的文章！

\end{document}
