\documentclass[12pt, twoside, a4paper, xetex]{article}
\usepackage{hcypaperstyle}

\begin{document}

\title{{\Huge C10K 问题\\}}
\author{黄丛宇\\06161032\\指导老师：马瑞芳}
\date{\today}

\maketitle

\lfour

	现在的web服务器同时要处理上万个客户端，你不这样认为么？毕竟，现在互联网是一个强大的地方。
	
	计算机也同样强大。你可以花1200美元购买一台有1000MHz的CPU，2GB内存和1000Mbit/秒以太网卡的机器。让我们来看，有20000个客户端，每个客户端可以得到50Hz的CPU，100Kbytes的内存和50Kbits每秒的带宽。这不会消耗太多的时间为20000个客户端中的每一个客户端每一秒从磁盘读取4Kbytes的数据然后通过网络发送给她们。（顺便说一句，每个客户端花费0.08美元。一些操作系统费用中，每个客户端100美元的许可费用看起来有点重了！）因此，硬件已经不是瓶颈。
	
	1999年，一个最繁忙的ftp站点，cdrom.com，确实同时处理10000个客户端通过一个万兆以太网络。到了2001年，多个ISP服务商提供同样的速度，期望cdrom.com能够持续增长，以成为她们的大商业客户。
	
	这时，瘦客户端的计算模型似乎以某种形式回来了---在互联网上，服务器为成千上万的客户端同时提供服务。
	
	考虑到这一点，在如何配置操作系统和如何编写代码来支持成千上万的客户端，这里有一些札记。由于是我感兴趣的领域，这些讨论围绕类Unix操作系统，当然，Windows也会提及一点。	

1 相关的网站

	在2003年十月，Felix von Leitner集中了一个优秀的网页并展示了关于网络的可扩展性，完成了比较不同网络系统调用和操作系统的基准。他的其中之一的结论就是Linux2.6内核完全击败了Linux2.4内核，同样，也有许多好的图表可以提供给操组系统开发者很好的思想源泉。（参看Slashdot的注解。看看一些人是怎样做后续基准程序来提高Felix的结果是非常有意思的。）

2 要先看的书。

	如果你还没有读过W. Richard Stevens的Unix网络编程：网络API：套接字联网（卷一），去读一遍吧。书中描述了很多关于编写高性能服务器的I/O策略和陷阱。书中甚至讨论'thundering herd'问题。如果你读完了，去看Jeff Darcy关于高性能服务器设计的札记。
（另一本Cal Henderson的构建可扩展的web站点（Building Scalable Web Sites）对那些用web服务器而不是写服务器的人更有帮助。）

3 I/O框架
	目前有关于下面展示的技术的预先包装的库提供，这些库可以使你的代码不依赖于操作系统，使其更具可移植性。
	（1）ACE，一个重量级的C++ I/O框架，包含一些I/O策略的面向对象的实现和一些其他有用的东西。在具体实践中，他的Reactor模式是处理非阻塞I/O的一种面向对象处理方式，Proactor则是处理异步I/O的面向对象方式。
	（2）ASIO是一个C++ I/O框架并且已经成为Boost库的一部分。它同ACE一样更新STL库。
	（3）libevent是Niels Provos编写的一个轻量级的C I/O框架。它支持kqueue和select，并且很快就会支持poll和epoll。它仅有水平触发模式，我认为有好处也有坏处。Niels有一个不错的时间图处理一个作为连接数量的函数的事件。它显示kqueue和sys_epoll是绝对的胜利者。
	（4）我自己的轻量级框架的尝试（很遗憾，没有持续更新）：
		（a）Poller是一个轻量级的C++ I/O框架，实现了使用任何你想要的依赖就绪API（poll，select，/dev/poll，kqueue或者sigio）的水平触发就绪API。它对于比较不同API的表现的基准很有用处。下面这个连接到Poller子类的文档展示了如何使用每一个就绪API。
		（b）rn是一个轻量级的C I/O框架，是我在Poller之后的第二次尝试。它是LGPL（因此更容易在社区程序中使用），用C编写（因此易于在非C++的程序中使用。）它在一些社区项目中被使用。
	（5）Matt Welsh在2000年四月份写了一篇关于在构建可扩展服务器是平衡工作线程和事件驱动技术的使用的论文。论文中涉及了Sandstorm I/O框架的一部分。
	（6）Cory Nelson's Scale!库--一个Windows上的非同步套接字，文件和管道的I/O库。

4 I/O策略
网络软件设计人员有很多选择。下面就是一些：
	（1）在一个单独的线程中是否处理多个I/O调用并且怎样处理。
		（a）绝对。始终使用阻塞/同步调用，并且可能的话，使用多线程或多进程实现并发。
		（b）使用非阻塞调用（例如：write()一个套接字时设置为O_NONBLOCK）来启动I/O，并且使用就绪通知机制（例如：poll()或者/dev/poll）来获得在那个通道上什么时候可以开始下一个I/O。
		（c）使用异步调用（例如：aio_write()）来开始I/O，并且使用完全通知机制（例如：信号或者完成端口）来获得I/O完成的时间。同时适用于网络和磁盘I/O。
	（2）怎样控制为每个客户端提供服务的代码。
		（a）每个客户端一个进程（典型的Unix解决方案，从1980年开始使用）
		（b）一个操作系统层面的线程处理很多客户端。每个客户端的控制者为：
			@ 一个用户层面的线程（例如：GNU状态线程，经典的Java绿色线程）
			@ 一个状态机（有点深奥，但是在一些地方很流行。我的最爱。）。
			@ 一个续集（有点深奥，但是同样在一些地方很流行）。
		（c）每个客户端一个操作系统层面的线程（例如：经典的Java本地线程）。
		（d）每个活动的客户端一个操作系统层面的线程（例如：Tomcat的apache前端，NT完成端口，线程池）。
	（3）是否使用标准的操作系统服务，或者把一些代码放到内核中。（例如：在一个第三方驱动，内核模块或者虚拟设备驱动（VxD））

下面的五种则和很流行：
	一个线程处理多个客户端，使用非阻塞I/O和水平触发就绪通知机制。
	一个线程处理多个客户端，使用非阻塞I/O和就绪状态改变通知机制。
	多个线程处理多个客户端，使用异步I/O。
	每个线程处理一个客户端，使用阻塞I/O。
	把服务器代码加到内核中。
	
4.1. 一个线程处理多个客户端，使用非阻塞I/O和水平触发就绪通知机制。

	将所有网络句柄设置成非阻塞模式，使用select()或者poll()来获知那个网络句柄有数据可读。这个是经典的传统模式。使用这种方案，操作系统内核告诉你那个文件描述符已经就绪，并且告诉你从最后一次内核告诉你这个事件是否你已经做了一些操作在那个文件描述符上。（名词“水平触发”来自计算机硬件设计。和“边际触发”相对。Jonathon Lemon在他的关于kqueue()的BSDCON 2000论文中介绍了这些。）
	
	注意：记住内核所发出的就绪通知仅仅是一个示意是很重要的。在你尝试从文件描述符读数据的时候，它可能压根就没有就绪。这就是为什么当使用就绪通知机制时使用非阻塞模式非常重要。
	这种方法的一个重要瓶颈是当请求的页此时不在核心上，read()或者sendfile()磁盘阻塞。将磁盘文件描述符设置成非阻塞没有什么作用。使用内存映射磁盘文件也一样。服务器第一次需要磁盘I/O时，它阻塞了，所有的客户端必须等待，这样原生非线程效率就被浪费了。
	
	这就是异步I/O要去解决的问题。但是在没有异步I/O的系统上，工作线程或进程进行磁盘I/O是依然会遇到这个瓶颈。一个解决方案是使用内存映射文件，如果mincore()标明需要I/O，调用一个工作线程去处理I/O，同时急促处理网络传输。Jef Poskanzer指出Pai, Druschel, 和Zwaenepoel的1999 Flash 网络服务器使用了这个方案。在Usenix '99他们有一个关于这个的讨论。似乎mincore()在一些BSD驱动的Unix，比如FreeBSD和Solaris上已经提供，但mincore()不是单一Unix标注的一部分。多亏Chuck Lever，mincore()作为Linux2.3.51内核的一部分提供。
	
	但是2003年11月在freebsd黑客列表中，Vivek Pei et al上报了一个不错的成果，他们利用系统剖析工具剖析它们的Flash Web服务器，然后再攻击其瓶颈。mincore是他们发现的其中之一的瓶颈（试想对所有情况那并不都是一个好办法）。另一个则是sendfile阻塞在磁盘访问上的事实。他们通过引进一个修改版的sendfile()来提高性能，这个修改版的sendfile()在当所需要获取的磁盘页不在核心上时，返回类似于EWOULDBLOCK的信息。（不能确定你如何告诉用户那个也现在在核心中了。。。在我看来，这里真正需要的是aio_sendfile()。）他们优化的结果是在一个1GHz/1GB FreeBSD盒子中，获得了SpecWeb99的800分。这要优于spec.org上其他任何在文件上的方案。

对于一个单线程，有多种方法可以获知在一个非阻塞的套接字集合中，那个套接字已经有I/O就绪：
	（a）传统的select().
	不幸的是，select()的FD_SETSIZE有限制。这个限制被编译到标准库和用户程序中。（一些版本的C库可以让你在用户程序编译期间提高这个限制。）请查看Poller_select(cc,h)关于如何使用select()和其他就绪通知方案互换的例子。
	（b）传统的poll()
	没有关于poll()所能处理的文件描述符的数量的硬编码的限制，但是由于在某一时间上，大多数的文件描述符处于等待状态并且扫描上千的文件描述符需要花费很多时间，当描述符的数量上千之后，它开始变慢。一些操作系统通过使用一些技术，比如轮询示意来提高poll()的速度。轮询示意在1999年被Niels Provos为Linux实现并成为标准。请查看Poller_poll(cc,h)关于如何使用poll()和其他就绪通知方案互换的例子。
	（c）/dev/poll
	这个是Solaris建议的poll的替代品。
	/dev/poll背后的思想是充分利用poll()经常被以相同的参数调用很多次的事实。通过/dev/poll，你得到一个打开的句柄。通过向这个句柄写数据，你只需要告诉操作系统一次你对什么文件感兴趣。之后，你只需要从那个句柄读一个当前就绪的文件描述符的集合。
	/dev/poll在Solaris7中悄然出现（查看106541补丁）。但是在Solaris8中才第一次公开出现。根据Sun的说法，在有750哥客户端时，/dev/poll比poll()有10%的效率提升。
	/dev/poll的不同实现都在Linux上试验过，但是没有一个性能可以epoll相当，并且没有一个实现真正完成。/dev/poll不建议在Linux上使用。
	请查看Poller_devpoll(cc, h 基准)关于如何使用/dev/poll和其他就绪通知方案互换的例子。（注意：这个例子是在Linux的/dev/poll，可能在Solaris下不能正确工作）。
	（d）kqueue()
	这个是FreeBSD建议的poll的替代品。（并且，也是NetBSD的）。下面指出，kqueue()可以设置成边际触发或者水平触发。

4.2. 每一个线程处理多个客户端，使用非阻塞I/O和就绪通知机制。

	就绪通知机制（或者边际触发就绪通知机制）表示你给内核一个文件描述符，之后，当这个描述符从没有就绪转换成就绪，内核在某个时候通知你。然后，操作系统内核就假设你已经知道那个文件描述符已经就绪。内核将不再发送那种类型的关于那个文件描述符的更多的就绪通知给你，直到你做了一些事情使文件描述符编程不就绪。（例如，直到你接受到一个EWOULDBLOCK错误在send，recv或者accept调用上，或者一次send或者recv传送的数据小于需要传送的字节数。）
	当你使用就绪改变通知机制时，你必须为假事件做好准备。因为一个通常的实现是当任何接受到数据包时就发送就绪信号，不管这个文件描述符是否就绪。
	这是“水平触发”就绪通知机制的对立面。由于你错过了仅仅一个事件，那么这个事件所对应的连接将永远处于等待状态，这是一个微小的编程错误疏忽。尽管如此，我发现边际触发就绪通知机制是使用OpenSSL的非阻塞客户端编程变得容易，因此，这值得去尝试。
	[Banga, Mogul, Drusha '99]在1999年描述了这种方案。
	有几种API可以是程序获得“文件描述符就绪”的通知：
	（a）kqueue()
	这是FreeBSD推荐的边际触发poll的替代品。（同样，很快也是NetBSD的）。
	FreeBSD4.3以及后续版本和NetBSD 2002年10月份的当前版本支持一个通用的poll()的候选者叫kqueue()/kevents()。支持边际触发和水平触发。（参见Jonathan Lemon's的主页和他的关于kqueue()的BSDCon 2000 论文。）
	和/dev/poll类似，你分配一个监听对象，但是不是通过打开文件/dev/poll，而是调用kqueue()去分配一个。为了改变你正在监听的事件，或者得到当前事件的列表，你在kqueue()返回的描述符上调用kevent()。它不仅可以监听套接字的就绪，还可以监听普通的文件描述符，信号，甚至I/O完成。
	注意：在2000年10月，FreeBSD的线程库不能很好的同kqueue()交互。显然，当kqueue()阻塞，整个进程阻塞，而不是调用的线程。
	参见Poller_kqueue（cc， h，基准）关于如何使用kqueue()和其他就绪通知机制互换的例子。
	使用kqueue()的例子和库：
	（1）PyKQueue--一个Python的kqueue()包转。
	（2）Ronald F. Guilmette的echo服务器的例子。参见他的2000年9月28日在freebsd.questions上的帖子。
	
	（b）epoll
	这是Linux 2.6内核推荐的边际触发poll的替代品。
	在2001年7月11日，Davide Libenzi建议了一个实时信号的候选者。他的补丁提供了他所谓的/dev/epoll www.xmailserver.org/linux-patches/nio-improve.html。这和实时信号就绪通知机制类似，但是他能合并冗余事件，并且有更高效的对付大批事件获得的机制。
	Epoll在他的接口从一个特殊的/dev中的文件改变成校内他调用，sys_epoll之后，以2.5.46合并到2.5内核树中。另外有为2.4内核提供的旧版本的epoll的补丁。
	2002年在linux内核邮件列表中，围绕Halloween有一个关于同一epoll，aio和其他别的事件资源的长时间的讨论。讨论也许不会在发生，但是Davide一直努力坚定epoll为通常情况下的首选。

	（c）
 
 
\end{document}
